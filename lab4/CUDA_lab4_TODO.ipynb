{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"1vdN-e34Q6LckBHPwbeZEv6OvQZehHelC","timestamp":1679313422584}],"collapsed_sections":["WoJbB3T5Vkw-","cReFlD-VRfZe","zs_a5Vuimily"],"toc_visible":true,"mount_file_id":"1mk0QXjREp-k_J-pdFfmgoC7-EVmgPyAo","authorship_tag":"ABX9TyOtJBmbLgKvAPmQvXd+wFAa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["---\n","# **LAB 4 - Unrolling e parallelismo dinamico**\n","---"],"metadata":{"id":"fZYqN0UwVLC_"}},{"cell_type":"markdown","metadata":{"id":"WoJbB3T5Vkw-"},"source":["# ▶️ CUDA setup"]},{"cell_type":"code","metadata":{"id":"Fht2Wy8wVkxJ"},"source":["!nvcc --version"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0jP2H_YJVkxJ"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## [GPU Compute Capability](https://developer.nvidia.com/cuda-gpus)"],"metadata":{"id":"VKbaxH9wWosO"}},{"cell_type":"markdown","metadata":{"id":"_cGSqZovVkxK"},"source":["## NVCC Plugin for Jupyter notebook\n","\n","*Usage*:\n","\n","\n","*   Load Extension `%load_ext nvcc_plugin`\n","*   Mark a cell to be treated as cuda cell\n","`%%cuda --name example.cu --compile false`\n","\n","**NOTE**: The cell must contain either code or comments to be run successfully. It accepts 2 arguments. `-n | --name` - which is the name of either CUDA source or Header. The name parameter must have extension `.cu` or `.h`. Second argument -c | --compile; default value is false. The argument is a flag to specify if the cell will be compiled and run right away or not. It might be usefull if you're playing in the main function\n","\n","*  We are ready to run CUDA C/C++ code right in your Notebook. For this we need explicitly say to the interpreter, that we want to use the extension by adding `%%cu` at the beginning of each cell with CUDA code. \n","\n","\n"]},{"cell_type":"code","metadata":{"id":"RCVhMkqYVkxK"},"source":["!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e6PDOytTVkxK"},"source":["%load_ext nvcc_plugin"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Bash and data setup"],"metadata":{"id":"cReFlD-VRfZe"}},{"cell_type":"markdown","source":["Clone GPUcomputing site on github..."],"metadata":{"id":"IYG8Cv4bTzyI"}},{"cell_type":"code","source":["!git clone https://github.com/giulianogrossi/GPUcomputing.git"],"metadata":{"id":"E7jZmHjCT0vu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ▶️ VS Code on Colab"],"metadata":{"id":"zs_a5Vuimily"}},{"cell_type":"code","source":["#@title Colab-ssh tunnel\n","#@markdown Execute this cell to open the ssh tunnel. Check [colab-ssh documentation](https://github.com/WassimBenzarti/colab-ssh) for more details.\n","\n","# Install colab_ssh on google colab\n","!pip install colab_ssh --upgrade\n","\n","from colab_ssh import launch_ssh_cloudflared, init_git_cloudflared\n","ssh_tunnel_password = \"gpu\" #@param {type: \"string\"}\n","launch_ssh_cloudflared(password=ssh_tunnel_password)\n","\n","# Optional: if you want to clone a Github or Gitlab repository\n","repository_url=\"https://github.com/giulianogrossi/GPUcomputing\" #@param {type: \"string\"}\n","init_git_cloudflared(repository_url)"],"metadata":{"id":"BCf9JxqphHAp","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iUYP4kCJhEIx"},"source":["# ✅ Occupancy Calculator - DeviceQuery"]},{"cell_type":"code","metadata":{"id":"NPaV4DfPh26Q"},"source":["%%cuda --name helper.h\n","// Beginning of GPU Architecture definitions\n","inline int _ConvertSMVer2Cores(int major, int minor) {\n","\t// Defines for GPU Architecture types (using the SM version to determine\n","\t// the # of cores per SM\n","\ttypedef struct {\n","\t\tint SM;  // 0xMm (hexidecimal notation), M = SM Major version,\n","\t\t// and m = SM minor version\n","\t\tint Cores;\n","\t} sSMtoCores;\n","\n","\tsSMtoCores nGpuArchCoresPerSM[] = {\n","\t\t\t{0x20, 32},\n","\t\t\t{0x30, 192},\n","\t\t\t{0x32, 192},\n","\t\t\t{0x35, 192},\n","\t\t\t{0x37, 192},\n","\t\t\t{0x50, 128},\n","\t\t\t{0x52, 128},\n","\t\t\t{0x53, 128},\n","\t\t\t{0x60,  64},\n","\t\t\t{0x61, 128},\n","\t\t\t{0x62, 128},\n","\t\t\t{0x70,  64},\n","\t\t\t{0x72,  64},\n","\t\t\t{0x75,  64},\n","\t\t\t{0x80,  64},\n","      {0x86, 128},\n","      {0x87, 128},\n","\t\t\t{-1, -1}};\n","\n","\tint index = 0;\n","\n","\twhile (nGpuArchCoresPerSM[index].SM != -1) {\n","\t\tif (nGpuArchCoresPerSM[index].SM == ((major << 4) + minor)) {\n","\t\t\treturn nGpuArchCoresPerSM[index].Cores;\n","\t\t}\n","\n","\t\tindex++;\n","\t}\n","\n","\t//# If we don't find the values, we default use the previous one to run properly\n","\tprintf(\n","\t\t\t\"MapSMtoCores for SM %d.%d is undefined.\"\n","\t\t\t\"  Default to use %d Cores/SM\\n\",\n","\t\t\tmajor, minor, nGpuArchCoresPerSM[index - 1].Cores);\n","\treturn nGpuArchCoresPerSM[index - 1].Cores;\n","}\n","\n","inline const char* _ConvertSMVer2ArchName(int major, int minor) {\n","  // Defines for GPU Architecture types (using the SM version to determine\n","  // the GPU Arch name)\n","  typedef struct {\n","    int SM;  // 0xMm (hexidecimal notation), M = SM Major version,\n","    // and m = SM minor version\n","    const char* name;\n","  } sSMtoArchName;\n","\n","  sSMtoArchName nGpuArchNameSM[] = {\n","      {0x30, \"Kepler\"},\n","      {0x32, \"Kepler\"},\n","      {0x35, \"Kepler\"},\n","      {0x37, \"Kepler\"},\n","      {0x50, \"Maxwell\"},\n","      {0x52, \"Maxwell\"},\n","      {0x53, \"Maxwell\"},\n","      {0x60, \"Pascal\"},\n","      {0x61, \"Pascal\"},\n","      {0x62, \"Pascal\"},\n","      {0x70, \"Volta\"},\n","      {0x72, \"Xavier\"},\n","      {0x75, \"Turing\"},\n","      {0x80, \"Ampere\"},\n","      {0x86, \"Ampere\"},\n","      {-1, \"Graphics Device\"}};\n","\n","  int index = 0;\n","\n","  while (nGpuArchNameSM[index].SM != -1) {\n","    if (nGpuArchNameSM[index].SM == ((major << 4) + minor)) {\n","      return nGpuArchNameSM[index].name;\n","    }\n","    index++;\n","  }\n","\n","  // If we don't find the values, we default use the previous one\n","  // to run properly\n","  printf(\n","      \"MapSMtoArchName for SM %d.%d is undefined.\"\n","      \"  Default to use %s\\n\",\n","      major, minor, nGpuArchNameSM[index - 1].name);\n","  return nGpuArchNameSM[index - 1].name;\n","}\n","// end of GPU Architecture definitions\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vvl8WXljg0WK"},"source":["%%cuda --name deviceQuery.cu\n","#include <stdlib.h>\n","#include <stdio.h>\n","#include \"helper.h\"\n","#include \"../GPUcomputing/utils/common.h\"\n","\n","int main(void) {\n","\n","\tprintf(\"\\nCUDA Device Query (Runtime API) version (CUDART static linking)\\n\\n\");\n","\tint deviceCount = 0;\n","\tCHECK(cudaGetDeviceCount(&deviceCount));\n","\n","\t// This function call returns 0 if there are no CUDA capable devices.\n","\tif (deviceCount == 0)\n","\t\tprintf(\"There are no available device(s) that support CUDA\\n\");\n","\telse\n","\t\tprintf(\"Detected %d CUDA Capable device(s)\\n\", deviceCount);\n","\n","\tint dev, driverVersion = 0, runtimeVersion = 0;\n","\n","\tfor (dev = 0; dev < deviceCount; ++dev) {\n","\t\tcudaSetDevice(dev);\n","\t\tcudaDeviceProp deviceProp;\n","\t\tcudaGetDeviceProperties(&deviceProp, dev);\n","\n","\t\tprintf(\"\\nDevice %d: \\\"%s\\\"\\n\", dev, deviceProp.name);\n","\n","\t\tcudaDriverGetVersion(&driverVersion);\n","\t\tcudaRuntimeGetVersion(&runtimeVersion);\n","\n","\t\tprintf(\"  CUDA Driver Version / Runtime Version          %d.%d / %d.%d\\n\",\n","\t\t\t\tdriverVersion / 1000, (driverVersion % 100) / 10,\n","\t\t\t\truntimeVersion / 1000, (runtimeVersion % 100) / 10);\n","\n","\t\tprintf(\"  GPU arch name:                                 %s\\n\",\n","\t\t\t\t\t\t_ConvertSMVer2ArchName(deviceProp.major, deviceProp.minor));\n","\n","\t\tprintf(\"  CUDA Capability Major/Minor version number:    %d.%d\\n\",\n","\t\t\t\tdeviceProp.major, deviceProp.minor);\n","\n","\t\tprintf(\"  Total amount of global memory:                 %.0f MBytes (%llu bytes)\\n\",\n","\t\t\t\t(float) deviceProp.totalGlobalMem / 1048576.0f,\n","\t\t\t\t(unsigned long long) deviceProp.totalGlobalMem);\n","\n","\t\tprintf(\"  (%2d) Multiprocessors, (%3d) CUDA Cores/MP:     %d CUDA Cores\\n\",\n","\t\t\t\t\t\tdeviceProp.multiProcessorCount,\n","\t\t\t\t\t\t_ConvertSMVer2Cores(deviceProp.major, deviceProp.minor),\n","\t\t\t\t\t\t_ConvertSMVer2Cores(deviceProp.major, deviceProp.minor) *\n","\t\t\t\t\t\t\t\tdeviceProp.multiProcessorCount);\n","\t\t\n","\t\tprintf(\"  GPU Max Clock rate:                            %.0f MHz (%0.2f GHz)\\n\",\n","\t\t\t\tdeviceProp.clockRate * 1e-3f, deviceProp.clockRate * 1e-6f);\n","\n","\t\tprintf(\"  Memory Clock rate:                             %.0f Mhz\\n\", deviceProp.memoryClockRate * 1e-3f);\n","\t\tprintf(\"  Memory Bus Width:                              %d-bit\\n\", deviceProp.memoryBusWidth);\n","\t\tif (deviceProp.l2CacheSize)\n","\t\t\tprintf(\"  L2 Cache Size:                                 %d bytes\\n\", deviceProp.l2CacheSize);\n","\n","\t\tprintf(\"  Maximum Texture Dimension Size (x,y,z)         1D=(%d), 2D=(%d, %d), 3D=(%d, %d, %d)\\n\",\n","\t\t\t\tdeviceProp.maxTexture1D, deviceProp.maxTexture2D[0],\n","\t\t\t\tdeviceProp.maxTexture2D[1], deviceProp.maxTexture3D[0],\n","\t\t\t\tdeviceProp.maxTexture3D[1], deviceProp.maxTexture3D[2]);\n","\n","\t\tprintf(\"  Maximum Layered 1D Texture Size, (num) layers  1D=(%d), %d layers\\n\",\n","\t\t\t\tdeviceProp.maxTexture1DLayered[0],\n","\t\t\t\tdeviceProp.maxTexture1DLayered[1]);\n","\n","\t\tprintf(\"  Maximum Layered 2D Texture Size, (num) layers  2D=(%d, %d), %d layers\\n\",\n","\t\t\t\tdeviceProp.maxTexture2DLayered[0],\n","\t\t\t\tdeviceProp.maxTexture2DLayered[1],\n","\t\t\t\tdeviceProp.maxTexture2DLayered[2]);\n","\n","\t\tprintf(\"  Total amount of constant memory                %lu bytes\\n\",\n","\t\t\t\tdeviceProp.totalConstMem);\n","\t\tprintf(\"  Total amount of shared memory per block        %lu bytes\\n\",\n","\t\t\t\tdeviceProp.sharedMemPerBlock);\n","\t\tprintf(\"  Total number of registers available per block  %d\\n\",\n","\t\t\t\tdeviceProp.regsPerBlock);\n","\t\tprintf(\"  Warp size                                      %d\\n\",\n","\t\t\t\tdeviceProp.warpSize);\n","\t\tprintf(\"  Maximum number of threads per multiprocessor   %d\\n\",\n","\t\t\t\tdeviceProp.maxThreadsPerMultiProcessor);\n","\t\tprintf(\"  Maximum number of threads per block            %d\\n\",\n","\t\t\t\tdeviceProp.maxThreadsPerBlock);\n","\t\tprintf(\"  Max dimension size of a thread block (x,y,z)  (%d, %d, %d)\\n\",\n","\t\t\t\tdeviceProp.maxThreadsDim[0], deviceProp.maxThreadsDim[1],\n","\t\t\t\tdeviceProp.maxThreadsDim[2]);\n","\t\tprintf(\"  Max dimension size of a grid size    (x,y,z)  (%d, %d, %d)\\n\",\n","\t\t\t\tdeviceProp.maxGridSize[0], deviceProp.maxGridSize[1],\n","\t\t\t\tdeviceProp.maxGridSize[2]);\n","\t\tprintf(\"  Maximum memory pitch                           %lu bytes\\n\",\n","\t\t\t\tdeviceProp.memPitch);\n","\t\tprintf(\"  Texture alignment                              %lu bytes\\n\",\n","\t\t\t\tdeviceProp.textureAlignment);\n","\t\tprintf(\"  Concurrent copy and kernel execution           %s with %d copy engine(s)\\n\",\n","\t\t\t\t(deviceProp.deviceOverlap ? \"Yes\" : \"No\"),\n","\t\t\t\tdeviceProp.asyncEngineCount);\n","\t\tprintf(\"  Run time limit on kernels                      %s\\n\",\n","\t\t\t\tdeviceProp.kernelExecTimeoutEnabled ? \"Yes\" : \"No\");\n","\t\tprintf(\"  Integrated GPU sharing Host Memory             %s\\n\",\n","\t\t\t\tdeviceProp.integrated ? \"Yes\" : \"No\");\n","\t\tprintf(\"  Support host page-locked memory mapping        %s\\n\",\n","\t\t\t\tdeviceProp.canMapHostMemory ? \"Yes\" : \"No\");\n","\t\tprintf(\"  Alignment requirement for Surfaces             %s\\n\",\n","\t\t\t\tdeviceProp.surfaceAlignment ? \"Yes\" : \"No\");\n","\t\tprintf(\"  Device has ECC support                         %s\\n\",\n","\t\t\t\tdeviceProp.ECCEnabled ? \"Enabled\" : \"Disabled\");\n","\n","\t\tprintf(\"  Device supports Unified Addressing (UVA):      %s\\n\",\n","\t\t\t\tdeviceProp.unifiedAddressing ? \"Yes\" : \"No\");\n","\t\tprintf(\"  Device PCI Domain ID / Bus ID / location ID:   %d / %d / %d\\n\",\n","\t\t\t\tdeviceProp.pciDomainID, deviceProp.pciBusID,\n","\t\t\t\tdeviceProp.pciDeviceID);\n","\t}\n","\treturn 0;\n","}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kW9b_Yuxi7id"},"source":["# Compilazione ed esecuzione\n","!ls\n","!nvcc src/deviceQuery.cu -o deviceQuery\n","!./deviceQuery"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Several API functions exist to assist programmers in choosing thread block size and cluster size based on register and shared memory requirements.\n","\n","- The occupancy calculator API, `cudaOccupancyMaxActiveBlocksPerMultiprocessor`, can provide an **occupancy prediction based on the block size and shared memory usage** of a kernel. This function reports occupancy in terms of the number of concurrent thread blocks per multiprocessor.\n","Note that this value can be converted to other metrics. Multiplying by the number of warps per block yields the number of concurrent warps per multiprocessor; further dividing concurrent warps by max warps per multiprocessor gives the occupancy as a percentage.\n","- The occupancy-based launch configurator APIs, `cudaOccupancyMaxPotentialBlockSize` and `cudaOccupancyMaxPotentialBlockSizeVariableSMem`, **heuristically calculate an execution configuration that achieves the maximum multiprocessor-level occupancy.**\n","- The occupancy calculator API, `cudaOccupancyMaxActiveClusters`, can provided occupancy prediction based on the cluster size, block size and shared memory usage of a kernel. This function reports occupancy in terms of number of max active clusters of a given size on the GPU present in the system."],"metadata":{"id":"evll6aSlzVtd"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n","\n","The following code sample calculates the occupancy of MyKernel. It then reports the occupancy level with the **ratio between concurrent warps versus maximum warps per multiprocessor**."],"metadata":{"id":"JSbDdWC3zzHX"}},{"cell_type":"code","source":["%%cuda --name occupancy.cu\n","#include <stdlib.h>\n","#include <stdio.h>\n","#include \"helper.h\"\n","#include \"../GPUcomputing/utils/common.h\"\n","\n","\n","// Device code\n","__global__ void MyKernel(int *d, int *a, int *b) {\n","  int idx = threadIdx.x + blockIdx.x * blockDim.x;\n","  d[idx] = a[idx] * b[idx];\n","}\n","\n","// Host code\n","int main() {\n","  int numBlocks;        // Occupancy in terms of active blocks\n","\n","  // These variables are used to convert occupancy to warps\n","  int device;\n","  cudaDeviceProp prop;\n","  int activeWarps;\n","  int maxWarps;\n","\n","  cudaGetDevice(&device);\n","  cudaGetDeviceProperties(&prop, device);\n","  maxWarps = prop.maxThreadsPerMultiProcessor / prop.warpSize;\n","\n","  for (int blockSize = 16; blockSize <= 1024; blockSize*=2) {\n","    cudaOccupancyMaxActiveBlocksPerMultiprocessor(&numBlocks, MyKernel, blockSize, 0);\n","    activeWarps = numBlocks * blockSize / prop.warpSize;\n","    double occup = (double)activeWarps / maxWarps * 100;\n","    printf(\"Occupancy [blockSize = %4d, activeWarps = %2d]:\\t%2.2f%%\\n\", blockSize, activeWarps, occup);\n","  }\n","  return 0;\n","}"],"metadata":{"id":"13Kv4NcSOanG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!nvcc -arch=sm_70 --ptxas-options=-v src/occupancy.cu -o occupancy\n","!./occupancy "],"metadata":{"id":"sXoLaamxSk33"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","The following code sample configures an occupancy-based kernel launch of MyKernel according to the user input.\n"],"metadata":{"id":"JoXN6tHHz5AV"}},{"cell_type":"code","source":["%%cuda --name occupancy.cu\n","#include <stdlib.h>\n","#include <stdio.h>\n","\n","\n","// Device code\n","__global__ void MyKernel(float *array, int arrayCount) {\n","    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n","    if (idx < arrayCount) {\n","        array[idx] *= array[idx];\n","    }\n","}\n","\n","__global__ void MyKernel1(int *d, int *a, int *b) {\n","  int idx = threadIdx.x + blockIdx.x * blockDim.x;\n","  d[idx] = a[idx] * b[idx];\n","}\n","\n","// Host code\n","int launchMyKernel(float *array, int arrayCount)\n","{\n","    int blockSize;      // The launch configurator returned block size\n","    int minGridSize;    // The minimum grid size needed to achieve the\n","                        // maximum occupancy for a full device\n","                        // launch\n","    int gridSize;       // The actual grid size needed, based on input\n","                        // size\n","\n","    cudaOccupancyMaxPotentialBlockSize(&minGridSize, &blockSize, (void*)MyKernel, 0, arrayCount);\n"," \n","    // Round up according to array size\n","    gridSize = (arrayCount + blockSize - 1) / blockSize;\n","    printf(\"blockSize = %4d, gridSize = %d, minGridSize = %2d\\n\", blockSize, gridSize, minGridSize);\n","\n","    MyKernel<<<gridSize, blockSize>>>(array, arrayCount);\n","    cudaDeviceSynchronize();\n","\n","    // occupancy calculator\n","    int device;\n","    int numBlocks;  \n","    cudaDeviceProp prop;\n","    cudaGetDevice(&device);\n","    cudaGetDeviceProperties(&prop, device);\n","    int maxWarps = prop.maxThreadsPerMultiProcessor / prop.warpSize;\n","    cudaOccupancyMaxActiveBlocksPerMultiprocessor(&numBlocks, MyKernel, blockSize, 0);\n","    int activeWarps = numBlocks * blockSize / 32;\n","    double occup = (double)activeWarps / maxWarps * 100;\n","    printf(\"Occupancy [blockSize = %4d, activeWarps = %2d]:\\t%2.2f%%\\n\", blockSize, activeWarps, occup);\n","\n","    return 0;\n","}\n","\n","// Host code\n","int main() {\n","  int N = 2<<20;\n","    size_t size = N * sizeof(float);\n","\n","    // Allocate input vectors h_A and h_B in host memory\n","    float* h_A = (float*)malloc(size);\n","    float* h_B = (float*)malloc(size);\n","    float* h_C = (float*)malloc(size);\n","  \n","    // Allocate vectors in device memory\n","    float* d_A;\n","    cudaMalloc(&d_A, size);\n","    float* d_B;\n","    cudaMalloc(&d_B, size);\n","    float* d_C;\n","    cudaMalloc(&d_C, size);\n","\n","    // Copy vectors from host memory to device memory\n","    cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);\n","    cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);\n","\n","    // Invoke kernel\n","    launchMyKernel(d_A, N);\n","\n","    // Free device memory\n","    cudaFree(d_A);\n","    cudaFree(d_B);\n","    cudaFree(d_C);\n","\n","\n","  return 0;\n","}"],"metadata":{"id":"o-tdfCu8LWfM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!nvcc -arch=sm_70 src/occupancy.cu -o occupancy\n","!./occupancy"],"metadata":{"id":"gr-0Ceo0LhaC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 🔴 TODO"],"metadata":{"id":"hlvdDnAdHaee"}},{"cell_type":"markdown","source":["Apply the same calculator to the previous kernel\n","\n","```\n","__global__ void blockParReduce1(int *in, int *out, ulong n)\n","```\n","\n"],"metadata":{"id":"lv1svOfqJFPr"}},{"cell_type":"code","source":["%%cuda --name occupancy1.cu\n","\n","\n","//TODO\n"],"metadata":{"id":"8BSlGWgvHaCl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!nvcc -arch=sm_75 src/occupancy1.cu -o occupancy1\n","!./occupancy1"],"metadata":{"id":"AckreRkvIawG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ✅ Profiling"],"metadata":{"id":"1snOeEsH_5z3"}},{"cell_type":"code","source":["%%cuda --name preduce.cu\n","#include <stdio.h>\n","#include <stdlib.h>\n","#include <assert.h>\n","\n","#include \"../GPUcomputing/utils/common.h\"\n","\n","\n","/*\n"," *  Block by block parallel implementation with divergence (sequential schema)\n"," */\n","__global__ void blockParReduce1(int *in, int *out, ulong n) {\n","\n","\tuint tid = threadIdx.x;\n","\tulong idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","\t// boundary check\n","\tif (idx >= n)\n","\t\treturn;\n","\n","\t// convert global data pointer to the local pointer of this block\n","\tint *thisBlock = in + blockIdx.x * blockDim.x;\n","\n","\t// in-place reduction in global memory\n","\tfor (int stride = 1; stride < blockDim.x; stride *= 2) {\n","\t\tif ((tid % (2 * stride)) == 0)\n","\t\t\tthisBlock[tid] += thisBlock[tid + stride];\n","\n","\t\t// synchronize within threadblock\n","\t\t__syncthreads();\n","\t}\n","\n","\t// write result for this block to global mem\n","\tif (tid == 0)\n","\t\tout[blockIdx.x] = thisBlock[0];\n","}\n","\n","/*\n"," *  Block by block parallel implementation without divergence (interleaved schema)\n"," */\n","__global__ void blockParReduce2(int *in, int *out, ulong n) {\n","\n","\tuint tid = threadIdx.x;\n","\tulong idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","\t// boundary check\n","\tif (idx >= n)\n","\t\treturn;\n","\n","\t// convert global data pointer to the local pointer of this block\n","\tint *thisBlock = in + blockIdx.x * blockDim.x;\n","\n","\t// in-place reduction in global memory\n","\tfor (int stride = blockDim.x / 2; stride > 0; stride >>= 1)  {\n","\t\tif (tid < stride)\n","\t\t\tthisBlock[tid] += thisBlock[tid + stride];\n","\n","\t\t// synchronize within threadblock\n","\t\t__syncthreads();\n","\t}\n","\n","\t// write result for this block to global mem\n","\tif (tid == 0)\n","\t\tout[blockIdx.x] = thisBlock[0];\n","}\n","\n","\n","/*\n"," * MAIN: test on parallel reduction\n"," */\n","int main(void) {\n","\tint *a, *b, *d_a, *d_b;\n","\tint blockSize = 1024;            // block dim 1D\n","\tulong numBlock = 1024*1024;      // grid dim 1D\n","\tulong n = blockSize * numBlock;  // array dim\n","\tlong sum_CPU = 0, sum_GPU;\n","\tlong nByte = n*sizeof(int), mByte = numBlock * sizeof(int);\n","\tdouble start, stopGPU, stopCPU, speedup;\n","\n","\tprintf(\"\\n****  test on parallel reduction  ****\\n\");\n","\n","\t// init\n","\ta = (int *) malloc(nByte);\n","\tb = (int *) malloc(mByte);\n","\tfor (ulong i = 0; i < n; i++) a[i] = 1;\n","\n","\tCHECK(cudaMalloc((void **) &d_a, nByte));\n","\tCHECK(cudaMemcpy(d_a, a, nByte, cudaMemcpyHostToDevice));\n","\tCHECK(cudaMalloc((void **) &d_b, mByte));\n","\tCHECK(cudaMemset((void *) d_b, 0, mByte));\n","\n","\t/***********************************************************/\n","\t/*                     CPU reduction                       */\n","\t/***********************************************************/\n","\tprintf(\"  Vector length: %.2f MB\\n\",n/(1024.0*1024.0));\n","\tprintf(\"\\n  CPU procedure...\\n\");\n","\tstart = seconds();\n","\tfor (ulong i = 0; i < n; i++) \n","    sum_CPU += a[i];\n","\tstopCPU = seconds() - start;\n","\tprintf(\"    Elapsed time: %f (sec) \\n\", stopCPU);\n","\tprintf(\"    sum: %lu\\n\",sum_CPU);\n","\n","\tprintf(\"\\n  GPU kernels (mem required %lu bytes)\\n\", nByte);\n","\n","\t/***********************************************************/\n","\t/*         KERNEL blockParReduce1 (divergent)              */\n","\t/***********************************************************/\n","\t// block by block parallel implementation with divergence\n","\tprintf(\"\\n  Launch kernel: blockParReduce1...\\n\");\n","\tstart = seconds();\n","\tblockParReduce1<<<numBlock, blockSize>>>(d_a, d_b, n);\n","\tCHECK(cudaGetLastError());\n","\tCHECK(cudaDeviceSynchronize());\n","\tstopGPU = seconds() - start;\n","\tspeedup = stopCPU/stopGPU;\n","\tprintf(\"    Elapsed time: %f (sec) - speedup %.1f\\n\", stopGPU,speedup);\n","\t\n","  // memcopy D2H\n","\tCHECK(cudaMemcpy(b, d_b, mByte, cudaMemcpyDeviceToHost));\n","\t\n","  // check result\n","\tsum_GPU = 0;\n","\tfor (uint i = 0; i < numBlock; i++)\n","\t\tsum_GPU += b[i];\n","\tassert(sum_GPU == n);\n","\n","\t// reset input vector on GPU\n","\tfor (ulong i = 0; i < n; i++) a[i]=1;\n","\tCHECK(cudaMemcpy(d_a, a, nByte, cudaMemcpyHostToDevice));\n","\n","\t/***********************************************************/\n","\t/*        KERNEL blockParReduce2  (non divergent)          */\n","\t/***********************************************************/\n","\t// block by block parallel implementation without divergence\n","\tprintf(\"\\n  Launch kernel: blockParReduce2...\\n\");\n","\tstart = seconds();\n","\tblockParReduce2<<<numBlock, blockSize>>>(d_a, d_b, n);\n","\tCHECK(cudaDeviceSynchronize());\n","\tstopGPU = seconds() - start;\n","\tspeedup = stopCPU/stopGPU;\n","\tprintf(\"    Elapsed time: %f (sec) - speedup %.1f\\n\", stopGPU,speedup);\n","\tCHECK(cudaGetLastError());\n","\t\n","  // memcopy D2H\n","\tCHECK(cudaMemcpy(b, d_b, mByte, cudaMemcpyDeviceToHost));\n","\t\n","  // check result\n","\tsum_GPU = 0;\n","\tfor (uint i = 0; i < numBlock; i++) {\n","\t\tsum_GPU += b[i];\n","  //\t\tprintf(\"b[%d] = %d\\n\",i,b[i]);\n","\t}\n","\tassert(sum_GPU == n);\n","\t\n","  // reset input vector on GPU\n","\tfor (ulong i = 0; i < n; i++) a[i] = 1;\n","\tCHECK(cudaMemcpy(d_a, a, nByte, cudaMemcpyHostToDevice));\n","\n","\t// check result\n","\tsum_GPU = 0;\n","\tfor (uint i = 0; i < numBlock; i++)\n","\t\tsum_GPU += b[i];\n","\tassert(sum_GPU == n);\n","\n","\tcudaFree(d_a);\n","\n","\tCHECK(cudaDeviceReset());\n","\treturn 0;\n","}"],"metadata":{"id":"URVoVFZajMAD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!nvcc -O2 -arch=sm_70 -lineinfo src/preduce.cu -o preduce\n","!ncu -o preduce_profile preduce\n","!nsys profile -o nsys-report preduce"],"metadata":{"id":"K8YvyTng_-iC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vXUIQkZLCTcG"},"source":["# ✅ Unrolling\n"]},{"cell_type":"code","metadata":{"id":"-Y52R0d3CA50"},"source":["%%cuda --name preduce.cu\n","#include <stdio.h>\n","#include <stdlib.h>\n","#include <assert.h>\n","\n","#include \"../GPUcomputing/utils/common.h\"\n","\n","\n","/*\n"," *  Block by block parallel implementation with divergence (sequential schema)\n"," */\n","__global__ void blockParReduce1(int *in, int *out, ulong n) {\n","\n","\tuint tid = threadIdx.x;\n","\tulong idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","\t// boundary check\n","\tif (idx >= n)\n","\t\treturn;\n","\n","\t// convert global data pointer to the local pointer of this block\n","\tint *thisBlock = in + blockIdx.x * blockDim.x;\n","\n","\t// in-place reduction in global memory\n","\tfor (int stride = 1; stride < blockDim.x; stride *= 2) {\n","\t\tif ((tid % (2 * stride)) == 0)\n","\t\t\tthisBlock[tid] += thisBlock[tid + stride];\n","\n","\t\t// synchronize within threadblock\n","\t\t__syncthreads();\n","\t}\n","\n","\t// write result for this block to global mem\n","\tif (tid == 0)\n","\t\tout[blockIdx.x] = thisBlock[0];\n","}\n","\n","/*\n"," *  Block by block parallel implementation without divergence (interleaved schema)\n"," */\n","__global__ void blockParReduce2(int *in, int *out, ulong n) {\n","\n","\tuint tid = threadIdx.x;\n","\tulong idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","\t// boundary check\n","\tif (idx >= n)\n","\t\treturn;\n","\n","\t// convert global data pointer to the local pointer of this block\n","\tint *thisBlock = in + blockIdx.x * blockDim.x;\n","\n","\t// in-place reduction in global memory\n","\tfor (int stride = blockDim.x / 2; stride > 0; stride >>= 1)  {\n","\t\tif (tid < stride)\n","\t\t\tthisBlock[tid] += thisBlock[tid + stride];\n","\n","\t\t// synchronize within threadblock\n","\t\t__syncthreads();\n","\t}\n","\n","\t// write result for this block to global mem\n","\tif (tid == 0)\n","\t\tout[blockIdx.x] = thisBlock[0];\n","}\n","\n","\n","/*\n"," * MAIN: test on parallel reduction\n"," */\n","int main(void) {\n","\tint *a, *b, *d_a, *d_b;\n","\tint blockSize = 1024;            // block dim 1D\n","\tulong numBlock = 1024*1024;      // grid dim 1D\n","\tulong n = blockSize * numBlock;  // array dim\n","\tlong sum_CPU = 0, sum_GPU;\n","\tlong nByte = n*sizeof(int), mByte = numBlock * sizeof(int);\n","\tdouble start, stopGPU, stopCPU, speedup;\n","\n","\tprintf(\"\\n****  test on parallel reduction  ****\\n\");\n","\n","\t// init\n","\ta = (int *) malloc(nByte);\n","\tb = (int *) malloc(mByte);\n","\tfor (ulong i = 0; i < n; i++) a[i] = 1;\n","\n","\tCHECK(cudaMalloc((void **) &d_a, nByte));\n","\tCHECK(cudaMemcpy(d_a, a, nByte, cudaMemcpyHostToDevice));\n","\tCHECK(cudaMalloc((void **) &d_b, mByte));\n","\tCHECK(cudaMemset((void *) d_b, 0, mByte));\n","\n","\t/***********************************************************/\n","\t/*                     CPU reduction                       */\n","\t/***********************************************************/\n","\tprintf(\"  Vector length: %.2f GB\\n\",n/(1024.0*1024.0*1024.0));\n","\tprintf(\"\\n  CPU procedure...\\n\");\n","\tstart = seconds();\n","\tfor (ulong i = 0; i < n; i++) \n","    sum_CPU += a[i];\n","\tstopCPU = seconds() - start;\n","\tprintf(\"    Elapsed time: %f (sec) \\n\", stopCPU);\n","\tprintf(\"    sum: %lu\\n\",sum_CPU);\n","\n","\tprintf(\"\\n  GPU kernels (mem required %lu bytes)\\n\", nByte);\n","\n","\t/***********************************************************/\n","\t/*         KERNEL blockParReduce1 (divergent)              */\n","\t/***********************************************************/\n","\t// block by block parallel implementation with divergence\n","\tprintf(\"\\n  Launch kernel: blockParReduce1...\\n\");\n","\tstart = seconds();\n","\tblockParReduce1<<<numBlock, blockSize>>>(d_a, d_b, n);\n","\tCHECK(cudaGetLastError());\n","\tCHECK(cudaDeviceSynchronize());\n","\tstopGPU = seconds() - start;\n","\tspeedup = stopCPU/stopGPU;\n","\tprintf(\"    Elapsed time: %f (sec) - speedup %.1f\\n\", stopGPU,speedup);\n","\t\n","  // memcopy D2H\n","\tCHECK(cudaMemcpy(b, d_b, mByte, cudaMemcpyDeviceToHost));\n","\t\n","  // check result\n","\tsum_GPU = 0;\n","\tfor (uint i = 0; i < numBlock; i++)\n","\t\tsum_GPU += b[i];\n","\tassert(sum_GPU == n);\n","\n","\t// reset input vector on GPU\n","\tfor (ulong i = 0; i < n; i++) a[i]=1;\n","\tCHECK(cudaMemcpy(d_a, a, nByte, cudaMemcpyHostToDevice));\n","\n","\t/***********************************************************/\n","\t/*        KERNEL blockParReduce2  (non divergent)          */\n","\t/***********************************************************/\n","\t// block by block parallel implementation without divergence\n","\tprintf(\"\\n  Launch kernel: blockParReduce2...\\n\");\n","\tstart = seconds();\n","\tblockParReduce2<<<numBlock, blockSize>>>(d_a, d_b, n);\n","\tCHECK(cudaDeviceSynchronize());\n","\tstopGPU = seconds() - start;\n","\tspeedup = stopCPU/stopGPU;\n","\tprintf(\"    Elapsed time: %f (sec) - speedup %.1f\\n\", stopGPU,speedup);\n","\tCHECK(cudaGetLastError());\n","\t\n","  // memcopy D2H\n","\tCHECK(cudaMemcpy(b, d_b, mByte, cudaMemcpyDeviceToHost));\n","\t\n","  // check result\n","\tsum_GPU = 0;\n","\tfor (uint i = 0; i < numBlock; i++) {\n","\t\tsum_GPU += b[i];\n","  //\t\tprintf(\"b[%d] = %d\\n\",i,b[i]);\n","\t}\n","\tassert(sum_GPU == n);\n","\t\n","  // reset input vector on GPU\n","\tfor (ulong i = 0; i < n; i++) a[i] = 1;\n","\tCHECK(cudaMemcpy(d_a, a, nByte, cudaMemcpyHostToDevice));\n","\n","\t// check result\n","\tsum_GPU = 0;\n","\tfor (uint i = 0; i < numBlock; i++)\n","\t\tsum_GPU += b[i];\n","\tassert(sum_GPU == n);\n","\n","\tcudaFree(d_a);\n","\n","\tCHECK(cudaDeviceReset());\n","\treturn 0;\n","}\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compilazione ed esecuzione\n","\n","!nvcc -arch=sm_70 src/preduce.cu -o preduce\n","!./preduce"],"metadata":{"id":"_nsVNk-gH5qe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!nvcc -arch=sm_70 src/nestedHelloWorld.cu -o nestedHelloWorld\n","!./nestedHelloWorld"],"metadata":{"id":"vP3_aRL8yVk7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 🔴 TODO"],"metadata":{"id":"4RYX2IU4dwaw"}},{"cell_type":"markdown","source":["Kernel privo di divergenza + unrolling:\n","\n","* Introdurre warp unrolling \n","* Introdurre block unrolling \n","* Specializzare su diversi num blocchi\n","* Confrontare i vari kernel... \n","\n","\n"],"metadata":{"id":"WK1z7xCcSz5v"}},{"cell_type":"code","source":["%%cuda --name preduceUnroll.cu\n","\n","#include <assert.h>\n","#include \"../GPUcomputing/utils/common.h\"\n","\n","\n","\n","/*\n"," *  Block by block parallel implementation with warp unrolling\n"," */\n","__global__ void blockParReduceUroll(int *in, int *out, ulong n) {\n","\n","\t\n","\t//TODO\n","}\n","\n","/*\n"," *  Multi block parallel implementation with block and warp unrolling\n"," */\n","__global__ void multBlockParReduceUroll8(int *in, int *out, ulong n) {\n","\n","\t\n","\t//TODO\n","}\n","\n","/*\n"," *  Multi block parallel implementation with block and warp unrolling\n"," */\n","__global__ void multBlockParReduceUroll16(int *in, int *out, ulong n) {\n","\n","\t\n","\t// TODO\n","}\n","\n","/*\n"," *  Block by block parallel implementation without divergence (interleaved schema)\n"," */\n","__global__ void blockParReduce2(int *in, int *out, ulong n) {\n","\n","\tuint tid = threadIdx.x;\n","\tulong idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","\t// boundary check\n","\tif (idx >= n)\n","\t\treturn;\n","\n","\t// convert global data pointer to the local pointer of this block\n","\tint *thisBlock = in + blockIdx.x * blockDim.x;\n","\n","\t// in-place reduction in global memory\n","\tfor (int stride = blockDim.x / 2; stride > 0; stride >>= 1)  {\n","\t\tif (tid < stride)\n","\t\t\tthisBlock[tid] += thisBlock[tid + stride];\n","\n","\t\t// synchronize within threadblock\n","\t\t__syncthreads();\n","\t}\n","\n","\t// write result for this block to global mem\n","\tif (tid == 0)\n","\t\tout[blockIdx.x] = thisBlock[0];\n","}\n","\n","\n","\n","/*\n"," * MAIN: test on parallel reduction\n"," */\n","int main(void) {\n","\tint *a, *b, *d_a, *d_b;\n","\tint blockSize = 1024;            // block dim 1D\n","\tulong numBlock = 1024*1024;      // grid dim 1D\n","\tulong n = blockSize * numBlock;  // array dim\n","\tlong sum_CPU = 0, sum_GPU;\n","\tlong nByte = n*sizeof(int), mByte = numBlock * sizeof(int);\n","\tdouble start, stopGPU, stopCPU, speedup;\n","\n","\tprintf(\"\\n****  test on parallel reduction  ****\\n\");\n","\n","\t// init\n","\ta = (int *) malloc(nByte);\n","\tb = (int *) malloc(mByte);\n","\tCHECK(cudaMalloc((void **) &d_a, nByte));\n","\tfor (ulong i = 0; i < n; i++) a[i] = 1;\n","\tCHECK(cudaMemcpy(d_a, a, nByte, cudaMemcpyHostToDevice));\n","\tCHECK(cudaMalloc((void **) &d_b, mByte));\n","\tCHECK(cudaMemset((void *) d_b, 0, mByte));\n","\n","\t/***********************************************************/\n","\t/*                     CPU reduction                       */\n","\t/***********************************************************/\n","\tprintf(\"  Vector length: %.2f GB\\n\",n/(1024.0*1024.0*1024.0));\n","\tprintf(\"\\n  CPU procedure...\\n\");\n","\tstart = seconds();\n","\tfor (ulong i = 0; i < n; i++) sum_CPU += a[i];\n","\tstopCPU = seconds() - start;\n","\tprintf(\"    Elapsed time: %f (sec) \\n\", stopCPU);\n","\tprintf(\"    sum: %lu\\n\",sum_CPU);\n","\n","\tprintf(\"\\n  GPU kernels (mem required %lu bytes)\\n\", nByte);\n","\n","\t/***********************************************************/\n","\t/*        KERNEL blockParReduce2  (non divergent)          */\n","\t/***********************************************************/\n","\t// block by block parallel implementation without divergence\n","\tprintf(\"\\n  Launch kernel: blockParReduce2...\\n\");\n","\tstart = seconds();\n","\tblockParReduce2<<<numBlock, blockSize>>>(d_a, d_b, n);\n","\tCHECK(cudaDeviceSynchronize());\n","\tstopGPU = seconds() - start;\n","\tspeedup = stopCPU/stopGPU;\n","\tprintf(\"    Elapsed time: %f (sec) - speedup %.1f\\n\", stopGPU,speedup);\n","\tCHECK(cudaGetLastError());\n","  // memcopy D2H\n","\tCHECK(cudaMemcpy(b, d_b, mByte, cudaMemcpyDeviceToHost));\n","  // check result\n","\tsum_GPU = 0;\n","\tfor (uint i = 0; i < numBlock; i++) {\n","\t\tsum_GPU += b[i];\n","\t}\n","\tassert(sum_GPU == n);\n","  // reset input vector on GPU\n","\tfor (ulong i = 0; i < n; i++) a[i] = 1;\n","\tCHECK(cudaMemcpy(d_a, a, nByte, cudaMemcpyHostToDevice));\n","\n","\t/***********************************************************/\n","\t/*               KERNEL blockParReduceUroll                */\n","\t/***********************************************************/\n","\t\n","\t// block by block parallel implementation without divergence\n","\tprintf(\"\\n  Launch kernel: blockParReduceUroll...\\n\");\n","\tstart = seconds();\n","\tblockParReduceUroll<<<numBlock, blockSize>>>(d_a, d_b, n);\n","\tCHECK(cudaDeviceSynchronize());\n","\tstopGPU = seconds() - start;\n","\tspeedup = stopCPU/stopGPU;\n","\tprintf(\"    Elapsed time: %f (sec) - speedup %.1f\\n\", stopGPU,speedup);\n","\tCHECK(cudaGetLastError());\n","\t// memcopy D2H\n","\tCHECK(cudaMemcpy(b, d_b, mByte, cudaMemcpyDeviceToHost));\n","\t// check result\n","\tsum_GPU = 0;\n","\tfor (uint i = 0; i < numBlock; i++)\n","\t\tsum_GPU += b[i];\n","\tassert(sum_GPU == n);\n","\t// reset input vector on GPU\n","\tfor (ulong i = 0; i < n; i++) a[i] = 1;\n","\tCHECK(cudaMemcpy(d_a, a, nByte, cudaMemcpyHostToDevice));\n","\n","\t/***********************************************************/\n","\t/*            KERNEL multBlockParReduceUroll8              */\n","\t/***********************************************************/\n","\t// block by block parallel implementation without divergence\n","\tprintf(\"\\n  Launch kernel: multBlockParReduceUroll8...\\n\");\n","\tstart = seconds();\n","\tmultBlockParReduceUroll8<<<numBlock/8, blockSize>>>(d_a, d_b, n);\n","\tCHECK(cudaDeviceSynchronize());\n","\tstopGPU = seconds() - start;\n","\tspeedup = stopCPU/stopGPU;\n","\tprintf(\"    Elapsed time: %f (sec) - speedup %.1f\\n\", stopGPU,speedup);\n","\tCHECK(cudaGetLastError());\n","\t// memcopy D2H\n","\tCHECK(cudaMemcpy(b, d_b, mByte, cudaMemcpyDeviceToHost));\n","\t// check result\n","\tsum_GPU = 0;\n","\tfor (uint i = 0; i < numBlock/8; i++)\n","\t\tsum_GPU += b[i];\n","\tprintf(\"    sum: %lu\\n\",sum_GPU);\n","\tassert(sum_GPU == n);\n","\t// reset input vector on GPU\n","\tfor (ulong i = 0; i < n; i++) a[i] = 1;\n","\tCHECK(cudaMemcpy(d_a, a, nByte, cudaMemcpyHostToDevice));\n","\n","\t/***********************************************************/\n","\t/*            KERNEL multBlockParReduceUroll16             */\n","\t/***********************************************************/\n","\t// block by block parallel implementation without divergence\n","\tprintf(\"\\n  Launch kernel: multBlockParReduceUroll16...\\n\");\n","\tstart = seconds();\n","\tmultBlockParReduceUroll16<<<numBlock/16, blockSize>>>(d_a, d_b, n);\n","\tCHECK(cudaDeviceSynchronize());\n","\tstopGPU = seconds() - start;\n","\tspeedup = stopCPU/stopGPU;\n","\tprintf(\"    Elapsed time: %f (sec) - speedup %.1f\\n\", stopGPU,speedup);\n","\tCHECK(cudaGetLastError());\n","\t// memcopy D2H\n","\tCHECK(cudaMemcpy(b, d_b, mByte, cudaMemcpyDeviceToHost));\n","\t// check result\n","\tsum_GPU = 0;\n","\tfor (uint i = 0; i < numBlock/16; i++)\n","\t\tsum_GPU += b[i];\n","\tassert(sum_GPU == n);\n","\n","\tcudaFree(d_a);\n","\n","\tCHECK(cudaDeviceReset());\n","\treturn 0;\n","}"],"metadata":{"id":"NAeBUVaScJKK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SOFMQZAkjlLW"},"source":["# ✅ Dynamic Parallelism"]},{"cell_type":"code","source":["%%cuda --name nestedHelloWorld.cu\n","#include <stdio.h>\n","\n","/*\n"," * A simple example of nested kernel launches from the GPU. Each thread displays\n"," * its information when execution begins, and also diagnostics when the next\n"," * lowest nesting layer completes.\n"," */\n","\n","__global__ void nestedHelloWorld(int const iSize, int iDepth) {\n","\tint tid = threadIdx.x;\n","\tprintf(\"Recursion=%d: Hello World from thread %d block %d\\n\", iDepth, tid,\tblockIdx.x);\n","\n","\t// condition to stop recursive execution\n","\tif (iSize == 1)\n","\t\treturn;\n","\n","\t// reduce block size to half\n","\tint nthreads = iSize >> 1;\n","\n","\t// thread 0 launches child grid recursively\n","\tif (tid == 0 && nthreads > 0) {\n","\t\tnestedHelloWorld<<<1, nthreads>>>(nthreads, ++iDepth);\n","\t\tprintf(\"-------> nested execution depth: %d\\n\", iDepth);\n","\t}\n","}\n","\n","int main(int argc, char **argv) {\n","\tint size = 8;\n","\tint blocksize = 8;   // initial block size\n","\tint igrid = 1;\n","\n","\tsize = igrid * blocksize;\n","\n","\tdim3 block(blocksize, 1);\n","\tdim3 grid((size + block.x - 1) / block.x, 1);\n","\tprintf(\"%s Execution Configuration: grid %d block %d\\n\", argv[0], grid.x,\n","\t\t\tblock.x);\n","\n","\tnestedHelloWorld<<<grid, block>>>(block.x, 0);\n","\n","\tcudaDeviceReset();\n","\treturn 0;\n","}"],"metadata":{"id":"WJQmiG5fyEqy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!nvcc -arch=sm_70 -dc src/nestedHelloWorld.cu \n","!nvcc -arch=sm_70 *.o -o nestedHelloWorld\n","!./nestedHelloWorld\n"],"metadata":{"id":"F4AkCagXzdAF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Introdurre il parallelismo dinamico nel calcolo di prodotti MQDB"],"metadata":{"id":"VR2PLOEZz4fD"}},{"cell_type":"markdown","source":["# 🔴 TODO"],"metadata":{"id":"Z2o3XANQf6Zv"}},{"cell_type":"code","source":["%%cuda --name mqdb.h \n","\n","#include <stdio.h>\n","#include <stdlib.h>\n","#include <string.h>\n","#include <math.h>\n","#include <sys/time.h>\n","#include <time.h>\n","\n","#ifndef MQDB_H\n","#define MQDB_H\n","\n","#define randu() ((float)rand() / (float) RAND_MAX)\n","#define abs(x) ((x)<0 ? (-x) : (x))\n","\n","typedef unsigned long ulong;\n","typedef unsigned int uint;\n","\n","typedef struct MQDB {\n","\tchar desc[100];   // description\n","\tint nBlocks;      // num. of blocks\n","\tint *blkSize;     // block dimensions\n","\tfloat *elem;       // elements in row-major order\n","\tulong nElems;     // actual number of elements\n","} mqdb;\n","\n","typedef unsigned long ulong;\n","typedef unsigned int uint;\n","\n","// # function prototypes #\n","int genRandDims(mqdb*, uint, uint);\n","void fillBlocks(mqdb*, uint, uint, char, float);\n","mqdb mqdbConst(uint, uint, uint, float);\n","void mqdbProd(mqdb, mqdb, mqdb);\n","void matProd(mqdb, mqdb, mqdb);\n","void checkResult(mqdb, mqdb);\n","void mqdbDisplay(mqdb);\n","\n","#endif"],"metadata":{"id":"lXx-e0jR3mL2"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QVQVpcvKjkIk"},"source":["%%cuda --name MQDB-CUDA-DP.cu\n","\n","//TODO\n","\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compilazione ed esecuzione\n","!nvcc -arch=sm_70 -dc src/MQDB-CUDA-DP.cu GPUcomputing/lab1/MQDB/mqdb.cpp \n","!nvcc -arch=sm_70 MQDB-CUDA-DP.o mqdb.o -o mqdb_prod\n","!rm -rf *.o\n","!./mqdb_prod"],"metadata":{"id":"_ci8xYBY2oSM"},"execution_count":null,"outputs":[]}]}